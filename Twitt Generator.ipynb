{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donald Trump Twitt Generator with KERAS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length: 2870760\n",
      "Loaded training data...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "\n",
    "# Text file containing words for training\n",
    "#training_file = 'train.txt'\n",
    "training_file = 'tweets_all.txt'\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [t for t in content if 'http' not in t]\n",
    "    content = [t for t in content if '&gt' not in t]\n",
    "    content = [x.strip() for x in content]\n",
    "    corpus = u' '.join(content)\n",
    "    \n",
    "    global CORPUS_LENGTH\n",
    "    \n",
    "    CORPUS_LENGTH = len(corpus)\n",
    "    print('Corpus Length:', CORPUS_LENGTH)\n",
    "        \n",
    "    content = [w.replace('\"',\"\") for w in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content, corpus\n",
    "\n",
    "training_data, corpus = read_data(training_file)\n",
    "\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "flat_list = []\n",
    "for sublist in training_data:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "flat_list\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique characters: 39\n"
     ]
    }
   ],
   "source": [
    "N_CHARS = None\n",
    "\n",
    "def create_index_char_map(corpus):\n",
    "    chars = sorted(list(set(corpus)))\n",
    "    global N_CHARS\n",
    "    N_CHARS = len(chars)\n",
    "    #if verbose:\n",
    "    print('No. of unique characters:', N_CHARS)\n",
    "    char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "    idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "    return chars, char_to_idx, idx_to_char\n",
    "\n",
    "chars, char_to_idx, idx_to_char = create_index_char_map(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sequences: 956900\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 60\n",
    "SEQ_STEP = 3\n",
    "N_SEQS = None\n",
    "\n",
    "def create_sequences(corpus):\n",
    "    sequences, next_chars = [], []\n",
    "    for i in range(0, CORPUS_LENGTH - MAX_SEQ_LENGTH, SEQ_STEP):\n",
    "        sequences.append(corpus[i:i + MAX_SEQ_LENGTH])\n",
    "        next_chars.append(corpus[i + MAX_SEQ_LENGTH])\n",
    "    global N_SEQS\n",
    "    N_SEQS = len(sequences)\n",
    "\n",
    "    print('No. of sequences:', len(sequences))\n",
    "    return np.array(sequences), np.array(next_chars)\n",
    "\n",
    "sequences, next_chars = create_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequences, next_chars, char_to_idx):\n",
    "    X = np.zeros((N_SEQS, MAX_SEQ_LENGTH, N_CHARS), dtype=np.bool)\n",
    "    y = np.zeros((N_SEQS, N_CHARS), dtype=np.bool)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for t, char in enumerate(sequence):\n",
    "            X[i, t, char_to_idx[char]] = 1\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1\n",
    "    return X, y\n",
    "\n",
    "X, y = one_hot_encode(sequences, next_chars, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 60, 128)           86016     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 39)                5031      \n",
      "=================================================================\n",
      "Total params: 222,631\n",
      "Trainable params: 222,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(hidden_layer_size=128, dropout=0.2, learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=True, input_shape=(MAX_SEQ_LENGTH, N_CHARS)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(hidden_layer_size, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(N_CHARS, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=learning_rate))\n",
    "    print('Model Summary:')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "956900/956900 [==============================] - 4061s 4ms/step - loss: 3.8169e-06\n",
      "Epoch 2/180\n",
      "956900/956900 [==============================] - 4087s 4ms/step - loss: 2.7683e-07\n",
      "Epoch 3/180\n",
      "956900/956900 [==============================] - 4071s 4ms/step - loss: 5.9175e-12\n",
      "Epoch 4/180\n",
      "956900/956900 [==============================] - 4073s 4ms/step - loss: 2.8653e-12\n",
      "Epoch 5/180\n",
      "956900/956900 [==============================] - 4074s 4ms/step - loss: 2.9526e-11\n",
      "Epoch 6/180\n",
      "956900/956900 [==============================] - 4112s 4ms/step - loss: 3.7374e-13\n",
      "Epoch 7/180\n",
      "956900/956900 [==============================] - 4136s 4ms/step - loss: 1.1212e-12\n",
      "Epoch 8/180\n",
      "956900/956900 [==============================] - 4140s 4ms/step - loss: 8.7205e-13\n",
      "Epoch 9/180\n",
      "956900/956900 [==============================] - 4141s 4ms/step - loss: 8.0976e-13\n",
      "Epoch 10/180\n",
      "956900/956900 [==============================] - 4173s 4ms/step - loss: 3.4882e-12\n",
      "Epoch 11/180\n",
      "956900/956900 [==============================] - 4166s 4ms/step - loss: 8.7205e-13\n",
      "Epoch 12/180\n",
      "956900/956900 [==============================] - 4156s 4ms/step - loss: 1.2458e-13\n",
      "Epoch 13/180\n",
      "956900/956900 [==============================] - 4153s 4ms/step - loss: 1.4949e-12\n",
      "Epoch 14/180\n",
      "956900/956900 [==============================] - 4226s 4ms/step - loss: 1.8687e-12\n",
      "Epoch 15/180\n",
      "956900/956900 [==============================] - 4275s 4ms/step - loss: 1.2458e-13\n",
      "Epoch 16/180\n",
      "956900/956900 [==============================] - 4198s 4ms/step - loss: 8.7205e-13\n",
      "Epoch 17/180\n",
      "956900/956900 [==============================] - 4100s 4ms/step - loss: 1.2458e-13\n",
      "Epoch 18/180\n",
      "956900/956900 [==============================] - 4098s 4ms/step - loss: 4.9831e-13\n",
      "Epoch 19/180\n",
      "112512/956900 [==>...........................] - ETA: 1:03:00 - loss: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "def train_model(model, X, y, batch_size = 128, nb_epoch = 180):\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='loss', save_best_only=True, mode='min')\n",
    "    model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, callbacks=[checkpointer])\n",
    "\n",
    "train_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 0.2\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet no. 001\n",
      "=============\n",
      "Generating with seed:\n",
      " truly enjoy your insite and opinions pl\n",
      "________________________________________\n",
      "Tweet no. 002\n",
      "=============\n",
      "Generating with seed:\n",
      " hard at work\" \"obama killed over 100k j\n",
      "________________________________________\n",
      "Tweet no. 003\n",
      "=============\n",
      "Generating with seed:\n",
      " \" circulation is way down and all he th\n",
      "________________________________________\n",
      "Tweet no. 004\n",
      "=============\n",
      "Generating with seed:\n",
      " in the same sentence as al sharpton lik\n",
      "________________________________________\n",
      "Tweet no. 005\n",
      "=============\n",
      "Generating with seed:\n",
      " blow their chance to take the senate mu\n",
      "________________________________________\n",
      "Tweet no. 006\n",
      "=============\n",
      "Generating with seed:\n",
      " a nice article in the new york times ab\n",
      "________________________________________\n",
      "Tweet no. 007\n",
      "=============\n",
      "Generating with seed:\n",
      " to be able to prosper again\" \"it is a s\n",
      "________________________________________\n",
      "Tweet no. 008\n",
      "=============\n",
      "Generating with seed:\n",
      " istheyre cowards mr trump i appreciate \n",
      "________________________________________\n",
      "Tweet no. 009\n",
      "=============\n",
      "Generating with seed:\n",
      " guy cant do a simple interviewsaw him t\n",
      "________________________________________\n",
      "Tweet no. 010\n",
      "=============\n",
      "Generating with seed:\n",
      " premiums increasing 33 in pennsylvania \n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "def generate_tweets(model, corpus, char_to_idx, idx_to_char, n_tweets = 10, verbose=0): \n",
    "    model.load_weights('weights.hdf5')\n",
    "    tweets = []\n",
    "    spaces_in_corpus = np.array([idx for idx in range(CORPUS_LENGTH) if corpus[idx] == ' '])\n",
    "    for i in range(1, n_tweets + 1):\n",
    "        begin = np.random.choice(spaces_in_corpus)\n",
    "        tweet = u''\n",
    "        sequence = corpus[begin:begin + MAX_SEQ_LENGTH]\n",
    "        tweet += sequence\n",
    "\n",
    "        print('Tweet no. %03d' % i)\n",
    "        print('=' * 13)\n",
    "        print('Generating with seed:')\n",
    "        print(sequence)\n",
    "        print('_' * len(sequence))\n",
    "        for _ in range(100):\n",
    "            x = np.zeros((1, MAX_SEQ_LENGTH, N_CHARS))\n",
    "            for t, char in enumerate(sequence):\n",
    "                x[0, t, char_to_idx[char]] = 1.0\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_idx = sample(preds)\n",
    "            next_char = idx_to_char[next_idx]\n",
    "\n",
    "            tweet += next_char\n",
    "            sequence = sequence[1:] + next_char\n",
    "        if verbose:\n",
    "            print(tweet)\n",
    "            print()\n",
    "        tweets.append(tweet)\n",
    "    return tweets\n",
    "\n",
    "tweets = generate_tweets(model, corpus, char_to_idx, idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03202742324370831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(sequences)\n",
    "Xval = vectorizer.transform(tweets)\n",
    "print(pairwise_distances(Xval, Y=tfidf, metric='cosine').min(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow -- In Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dictionary)\n",
    "\n",
    "# number of units in RNN cell\n",
    "num_hidden = 128 #512\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 1000\n",
    "display_step = 100\n",
    "n_input = 3\n",
    "timesteps = 3\n",
    "batch_size = 3\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {'out': tf.Variable(tf.random_normal([num_hidden, vocab_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_size]))}\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "#x = tf.placeholder(tf.float32, (None, None, 3)) \n",
    "y = tf.placeholder(\"float\", [None, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "    \n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "    #rnn_cell = rnn.BasicLSTMCell(num_hidden)\n",
    "    #initial_state = rnn_cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(num_hidden*4),rnn.BasicLSTMCell(num_hidden*2),\n",
    "                                rnn.BasicLSTMCell(num_hidden),rnn.BasicLSTMCell(num_hidden)])\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = tf.nn.static_rnn(rnn_cell, inputs = x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "#pred = BiRNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_embedding_layer(inputs_, vocab_size, embed_size):\n",
    "#    \"\"\"\n",
    "#    Create the embedding layer\n",
    "#    \"\"\"\n",
    "#    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_size), -1, 1))\n",
    "#    embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "    \n",
    "#def build_lstm_layers(lstm_sizes, embed, keep_prob_, batch_size):\n",
    "#    \"\"\"\n",
    "#    Create the LSTM layers\n",
    "#    \"\"\"\n",
    "    \n",
    "#    lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "#    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "#    cell = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "#    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "#    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "    \n",
    "#    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "#pred = build_lstm_layers(inputs_, vocab_size, embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 100, Average Loss= 10.313104, Average Accuracy= 0.00%\n",
      "['rest', 'until', 'the'] - [job] vs [prepaid]\n",
      "Iter= 200, Average Loss= 10.375281, Average Accuracy= 0.00%\n",
      "['do', 'we', 'work'] - [so] vs [welp]\n",
      "Iter= 300, Average Loss= 10.075018, Average Accuracy= 1.00%\n",
      "['s', 'and', 'attorney'] - [baker] vs [the]\n",
      "Iter= 400, Average Loss= 9.641488, Average Accuracy= 6.00%\n",
      "['god', 'bless', 'you'] - [and] vs [the]\n",
      "Iter= 500, Average Loss= 9.156170, Average Accuracy= 4.00%\n",
      "['her', 'about', 'an'] - [affair] vs [the]\n",
      "Iter= 600, Average Loss= 8.576650, Average Accuracy= 5.00%\n",
      "['war', 'negotiations', 'going'] - [on] vs [is]\n",
      "Iter= 700, Average Loss= 8.717513, Average Accuracy= 7.00%\n",
      "['more', 'representative', 'important'] - [and] vs [the]\n",
      "Iter= 800, Average Loss= 8.819504, Average Accuracy= 6.00%\n",
      "['for', 'this', 'kind'] - [of] vs [to]\n",
      "Iter= 900, Average Loss= 8.692910, Average Accuracy= 3.00%\n",
      "['would', 'have', 'been'] - [a] vs [the]\n",
      "Iter= 1000, Average Loss= 8.796187, Average Accuracy= 2.00%\n",
      "['history', 'culture', 'and'] - [destiny] vs [of]\n",
      "3 words: \"donald trump is\"\n",
      "donald trump is of of of of of of of of of of\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "step = 0\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        if offset > (len(flat_list)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "            \n",
    "        symbols_in_keys = [ [dictionary[ str(flat_list[i])]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(flat_list[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "    \n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [flat_list[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = flat_list[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "        \n",
    "    prompt = \"%s words: \" % n_input\n",
    "    sentence = input(prompt)\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "        \n",
    "    symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "        \n",
    "    for i in range(10):\n",
    "        keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "        symbols_in_keys = symbols_in_keys[1:]\n",
    "        symbols_in_keys.append(onehot_pred_index)\n",
    "    print(sentence)\n",
    "            \n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iter= 50000, Average Loss= 0.433600, Average Accuracy= 91.70%\n",
    "#\"good morning jordan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
